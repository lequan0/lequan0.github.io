---
title: "Lab 20"
author: "quan le"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Random Forests
```{r}
library(caret)
library(titanic)
library(GGally)
library(randomForest)
library(yardstick)

titanic_train$Survived = as.factor(titanic_train$Survived)

titanic_train %>% 
  ggpairs(columns = c("Pclass",
                      "Sex",
                      "Age",
                      "Fare"),
          mapping = aes(color = Survived))

# Let us split into training and test sets
## 75% of the sample size
train.index <- createDataPartition(titanic_train$Survived, p = .75, list = FALSE)
train <- titanic_train[ train.index,]
test  <- titanic_train[-train.index,]
```



```{r}
###################
# Random forest
# Need to take care of NAs
rfModel1 <- randomForest(Survived~Fare + Age,
                        data=train,
                        na.action=na.omit)

print(rfModel1)
surv_pred <- predict(rfModel1, newdata=test, type='class')

trueAndPredFr <- data.frame(surv_pred, test$Survived)
confMat <- conf_mat(trueAndPredFr, truth=test.Survived, estimate=surv_pred)

autoplot(confMat, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  theme(legend.position = "right")
```


```{r}
############################
# What is the complexity parameter here?
rfModel2 <- randomForest(Survived~Pclass + Age,
                        data=train,
                        na.action=na.omit,
                        ntree=10)

print(rfModel2)
surv_pred <- predict(rfModel2, newdata=test, type='class')

trueAndPredFr <- data.frame(surv_pred, test$Survived)
confMat <- conf_mat(trueAndPredFr, truth=test.Survived, estimate=surv_pred)

autoplot(confMat, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  theme(legend.position = "right")
```


```{r}
##########################
# Decision boundary for random forest

# Function for plotting decision boundary
# Taken from https://michael.hahsler.net/SMU/EMIS7332/R/viz_classifier.html
decisionplot <- function(model, data, class = NULL, predict_type = "class",
                         resolution = 100, showgrid = TRUE, ...) {
  
  if(!is.null(class)) cl <- data[,class] else cl <- 1
  data <- data[,1:2]
  k <- length(unique(cl))
  
  plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
  
  # make grid
  r <- sapply(data, range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as.data.frame(g)
  
  ### guess how to get class labels from predict
  ### (unfortunately not very consistent between models)
  p <- predict(model, g, type = predict_type)
  if(is.list(p)) p <- p$class
  p <- as.factor(p)
  
  if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
  
  z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
  contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
          lwd = 2, levels = (1:(k-1))+.5)
  
  invisible(z)
}

decisionplot(rfModel1, train[c("Fare", "Age", "Survived")], class = "Survived", main = "Random Forest")

decisionplot(rfModel2, train[c("Pclass", "Age", "Survived")], class = "Survived", main = "Random Forest")
```
 
 
## Feature Importance
```{r}
# What is the complexity parameter here?
rfModel3 <- randomForest(Survived~.,
                        data=train,
                        na.action=na.omit,
                        ntree=10)

print(rfModel3)
surv_pred <- predict(rfModel3, newdata=test, type='class')

trueAndPredFr <- data.frame(surv_pred, test$Survived)
confMat <- conf_mat(trueAndPredFr, truth=test.Survived, estimate=surv_pred)

autoplot(confMat, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1") +
  theme(legend.position = "right")

importance(rfModel3)
```
 
 