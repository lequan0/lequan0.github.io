---
title: "Lab 10"
author: "quan le"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```


```{r}
library(MASS)
library(tidyverse)
library(naivebayes)
library(ggplot2)
library(datasets)
library(interactions)
library(foreign)
library(nnet)
library(reshape2)
library(rsample)
library(dplyr)
library(caret)
library(e1071)
library(glmnet)
```




# Logistic Regression

```{r}
n = 500
p = 1
x = cbind(1, sort(rnorm(n)))
B = rbind(c(0, 1), c(0, 2), c(1, 2), c(-1, 2))
for (i in 1:nrow(B)) {
  y = rbinom(n, 1, 1 / (1 + exp(-x %*% B[i,]))) 
  fit = glm(y ~ x, family = "binomial")
  
  plot(x[,2], y, main=paste("beta =", B[i,2], "beta_0 =", B[i,1]))
  lines(x[,2], fit$fitted.values)
}
```


```{r}
n = 1000
p = 2
mu = rep(0, p); Sigma = diag(p)

x = mvrnorm(n, mu, Sigma)
b = c(2, 1)
y = rbinom(n, 1, 1 / (1 + exp(-x %*% b))) 

x = x %>% as.data.frame() %>% 
  mutate(class = y) %>%
  rename(X1 = V1, X2 = V2)
```

```{r}
# Sample data
ggplot(x) +
  geom_point(
    aes(x = X1, y = X2, color = factor(class), shape = factor(class)),
    alpha = 0.7
  ) +
  coord_fixed(ratio = 1) +
  scale_color_brewer(type = "div", palette = "Dark2") +
  labs(title = "Sample Data")
```

```{r}
x1p = seq(-3, 3, by = 0.05)
x2p = seq(-3, 3, by = 0.05)
zp <- expand.grid(X1 = x1p, X2 = x2p)

fit = glm(class ~ ., family = "binomial", data = x)
yhat = predict(fit, newdata = zp, type = "response")
z <- zp %>% 
  bind_cols(data.frame(p1 = yhat)) 
```

```{r}
ggplot(z) +
  geom_point(
    data = x,
    aes(x = X1, y = X2, color = factor(class), shape = factor(class)),
    alpha = 0.7
  ) + 
  geom_contour(
    aes(x = X1, y = X2, z = p1), 
    color    = "black",
    breaks   = 0.5
  ) +
  coord_fixed(ratio = 1) +
  scale_color_brewer(type = "div", palette = "Dark2") + 
  labs(title = "Decision Boundary for Logistic Regression")
```

```{r}
# Classifying email as spam!!!

# reading in data
data = read.csv("data/spam_dat.csv",header=FALSE)
ndat = read.delim("data/spam_vars.txt",header=FALSE)

# parsing variable names
nams = NULL
for(i in 1:nrow(ndat))
{
  vec = strsplit(as.character(ndat[i,]),split="_")
  for(j in 1:length(vec[[1]]))
  {
    if(length(grep(":",vec[[1]][j]))>0)
    {
      vars = strsplit(vec[[1]][j],split=":")
      nams = c(nams,vars[[1]][1])
    }
  }
}

Y = data[,58]
n = length(Y)
sum(Y)/n

X = as.matrix(log(1 + data[,1:57]))
colnames(X) = nams
X = scale(X)/sqrt(n-1)
dat = data.frame(Y,X)

# taking a subset of data
sdat = data.frame(Y,dat$george,dat$meeting,dat$total,dat$re,dat$edu,dat$free,dat$your)
plot(sdat,pch=16,cex=.5)

#########
# fit logistic model

fits = glm(Y~.,data=sdat,family="binomial",maxit=50)
summary(fits)
```


```{r}
# individual variable total
fit = glm(Y~dat.total,data=sdat,family="binomial")
summary(fit)
plot(sdat$dat.total,sdat$Y)
xs = seq(min(sdat$dat.total),max(sdat$dat.total),l=1000)
pihat = exp(fit$coefficients[1] + xs*fit$coefficients[2])/(1 + exp(fit$coefficients[1] + xs*fit$coefficients[2]))
lines(xs,pihat)
lines(c(min(xs),max(xs)),c(.5,.5),lty=2)
```


```{r}
#individual variable george
fit = glm(Y~dat.george,data=sdat,family="binomial")
summary(fit)
plot(sdat$dat.george,sdat$Y)
xs = seq(min(sdat$dat.george),max(sdat$dat.george),l=1000)
pihat = exp(fit$coefficients[1] + xs*fit$coefficients[2])/(1 + exp(fit$coefficients[1] + xs*fit$coefficients[2]))
lines(xs,pihat)
lines(c(min(xs),max(xs)),c(.5,.5),lty=2)
```


```{r}
#individual variable free
fit = glm(Y~dat.free,data=sdat,family="binomial")
summary(fit)
plot(sdat$dat.free,sdat$Y)
xs = seq(min(sdat$dat.free),max(sdat$dat.free),l=1000)
pihat = exp(fit$coefficients[1] + xs*fit$coefficients[2])/(1 + exp(fit$coefficients[1] + xs*fit$coefficients[2]))
lines(xs,pihat)
lines(c(min(xs),max(xs)),c(.5,.5),lty=2)
```

# Multinomial Regression


```{r}
n = 1000
p = 2
mu = rep(0, p); Sigma = diag(p)
x = mvrnorm(n, mu, 10*Sigma)
b1 = c(1, 0)
b2 = c(0, 1)
b3 = c(-1, -1)

pmulti = function(x, b) {
  return(exp(-x %*% b) / (exp(-x %*% b1) + exp(-x %*% b2) + exp(-x %*% b3)))
}

p = cbind(pmulti(x, b1), pmulti(x, b2), pmulti(x, b3))
y = apply(p, 1, function(x) sample(3, 1, prob=x))

xy = x %>% as.data.frame() %>% 
  mutate(class = y) %>%
  rename(X1 = V1, X2 = V2)


# Sample data
ggplot(xy) +
  geom_point(
    aes(x = X1, y = X2, color = factor(class), shape = factor(class)),
    alpha = 0.7
  ) +
  coord_fixed(ratio = 1) +
  scale_color_brewer(type = "div", palette = "Dark2") +
  labs(title = "Sample Data")



```


```{r}
x1p = seq(-10, 10, by = 0.1)
x2p = seq(-10, 10, by = 0.1)
zp <- expand.grid(X1 = x1p, X2 = x2p)

fit = glmnet(x, y, family = "multinomial", lambda = 0)
yhat = predict(fit, newx = as.matrix(zp), type = "response")
yhat = matrix(as.matrix(yhat), nrow = 40401, byrow=FALSE)

z <- zp %>% 
  bind_cols(data.frame(p1 = yhat[,1], p2 = yhat[,2], p3 = yhat[,3])) 

ggplot(z) +
  geom_point(
    data = xy,
    aes(x = X1, y = X2, color = factor(class), shape = factor(class)),
    alpha = 0.7
  ) +
  geom_contour(
    aes(x = X1, y = X2, z = p1),
    color    = "black",
    breaks   = 0.5
  ) +
  geom_contour(
    aes(x = X1, y = X2, z = p2),
    color    = "black",
    breaks   = 0.5
  ) +
  geom_contour(
    aes(x = X1, y = X2, z = p3),
    color    = "black",
    breaks   = 0.5
  ) +
  coord_fixed(ratio = 1) +
  scale_color_brewer(type = "div", palette = "Dark2") +
  labs(title = "Decision Boundary for Multinomial")
```




```{r}
######################################
# Multinomial Logistic regression
# hsbdemo data consists of different variables for 200 students
# 'prog' = program type (general, academic, or vocation) - this is the outcome variable
# 'ses' = economic status (ordinal/categorical)
# 'write' = score on a writing test (continuous)
# We will use 'ses' and 'write' as predictors

ml <- read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
with(ml, do.call(rbind, tapply(write, prog, function(x) c(M = mean(x), SD = sd(x)))))

# Specify prog == academic as baseline
ml$prog2 <- relevel(ml$prog, ref = "academic")
test <- multinom(prog2 ~ ses + write, data = ml)

summary(test)

# Interpretation: A one-unit increase in the variable write is associated with 
# the decrease in the log odds of being in general program vs. academic program 
# in the amount of .058

# Interpretation: A one-unit increase in the variable write is associated with 
# the decrease in the log odds of being in vocation program vs. academic program
# in the amount of .1136

# Interpretation: The log odds of being in general program vs. in academic 
# program will decrease by 1.163 if moving from ses="low" to ses="high"

# Interpretation: The log odds of being in vocation program vs. in academic 
# program will decrease by 0.983 if moving from ses="low" to ses="high".

# Others are not significant
```


```{r}
# Calculate p-values using Wald's test (2-tailed z-test)
z <- summary(test)$coefficients/summary(test)$standard.errors
z
```


```{r}
# 2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p
```


```{r}
# The ratio of the probability of choosing one outcome category over the 
# probability of choosing the baseline category is called relative risk (or odds).

# Calculate odds or relative risk
exp(coef(test))

# Interpretation: The relative risk ratio for a one-unit increase in the 
# variable write is .9437 for being in general program vs. academic program.

# Interpretation: The relative risk ratio switching from ses = 1 to 3 is .3126 
# for being in general program vs. academic program.
```


```{r}
# Predictions
# if we want to examine the changes in predicted probability associated with one
# of our two variables, we can create small datasets varying one variable while 
# holding the other constant. We will first do this holding write at its mean 
# and examining the predicted probabilities for each level of ses.

dses <- data.frame(ses = c("low", "middle", "high"), write = mean(ml$write))
predict(test, newdata = dses, "probs")
```


```{r}
# Another way to understand the model using the predicted probabilities is to 
# look at the averaged predicted probabilities for different values of the 
# continuous predictor variable write within each level of ses.

dwrite <- data.frame(ses = rep(c("low", "middle", "high"), each = 41), write = rep(c(30:70), 3))
```


```{r}
## store the predicted probabilities for each value of ses and write
pp.write <- cbind(dwrite, predict(test, newdata = dwrite, type = "probs", se = TRUE))

## calculate the mean probabilities within each level of ses
by(pp.write[, 3:5], pp.write$ses, colMeans)
```


```{r}
# Plot predicted probabilities for each level of ses
lpp <- melt(pp.write, id.vars = c("ses", "write"), value.name = "probability")
head(lpp)
```


```{r}
ggplot(lpp, aes(x = write, y = probability, colour = ses)) + geom_line() + facet_grid(variable ~., scales = "free")
```

# Penalized Logistic + Multinomial Regression

```{r}
fit.l1 = cv.glmnet(as.matrix(sdat[,-1]), sdat$Y, family="binomial", type.measure = "class")
fit.l1$glmnet.fit$beta[,match(fit.l1$lambda.1se, fit.l1$lambda)]
plot(fit.l1$glmnet.fit)
plot(fit.l1)
```

```{r}
fit.l1 = cv.glmnet(as.matrix(sdat[,-1]), sdat$Y, family="binomial", type.measure = "class")
fit.l1$glmnet.fit$beta[,match(fit.l1$lambda.1se, fit.l1$lambda)]
plot(fit.l1$glmnet.fit)
plot(fit.l1)
```



# Poisson Regression

```{r}
######################################
# Poisson regression
# Used for modeling "counts"
# "Link" is log - it is also called "log-linear model"
# log(y) = b0 + b1*x1 + b2*x2 + ... + bn*xn

# We will use warpbreaks dataset in R
# " This data set gives the number of warp breaks per loom, where a loom corresponds to a fixed length of yarn. "
# https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/warpbreaks.html

summary(warpbreaks)
```


```{r}
columns <- names(warpbreaks)
columns
```


```{r}
# Have alook at the dataset!
ls.str(warpbreaks)
```


```{r}
hist(warpbreaks$breaks)
```


```{r}
mean(warpbreaks$breaks)
var(warpbreaks$breaks)
```


```{r}
# Poisson regression
poisson.model <- glm(breaks ~ wool + tension, warpbreaks, family = poisson(link = "log"))
summary(poisson.model)
```


```{r}
# Note how categorical variables are converted into "dummy variables"
# Notice that wool==A and tension==L are treated as base categories
# Consider the beta for woolB, i.e., -0.20599, or for better interpretation, 
# we need exp(beta), i.e. exp(-0.20599) = 0.8138
# Interpretation: Changing from wool type A to wool type B results in a decrease
# in breaks equal to 0.8138 times the intercept, or, in other words,
# # breaks will fall by 18.6% if other variables are constant.

# Predicting using this model!
# make a dataframe with new data
newdata = data.frame(wool = "B", tension = "M")

# use 'predict()' to run model on new data
predict(poisson.model, newdata = newdata, type = "response")
# Interpretation: There will be about 24 breaks if we use wool B with tension M
```


```{r}
# Some interesting visualizations
cat_plot(poisson.model, pred = wool, modx = tension)

cat_plot(poisson.model, pred = tension, modx = wool, geom = "line")

cat_plot(poisson.model, pred = tension, modx = wool, geom = "line", plot.points = TRUE)
```